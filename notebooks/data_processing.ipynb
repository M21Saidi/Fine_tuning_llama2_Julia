{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "IZWP7d-Jpmok"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXcY9T5Cnkr-"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import csv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Leet Code Scrapping"
      ],
      "metadata": {
        "id": "tQea3KmkoO9L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solved problems"
      ],
      "metadata": {
        "id": "OqW9GIudohNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/JuliaCN/LeetCode.jl"
      ],
      "metadata": {
        "id": "NPyW-lmFoUXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the directory containing the files\n",
        "directory = \"/content/LeetCode.jl/src/problems\"\n",
        "\n",
        "# Create or open the CSV file\n",
        "csv_file_path = \"Julia_leet.csv\"\n",
        "with open(csv_file_path, mode='w', newline='', encoding='utf-8') as csv_file:\n",
        "    fieldnames = ['statement', 'solution']\n",
        "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "\n",
        "    # Iterate through each file in the directory\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".jl\"):\n",
        "            file_path = os.path.join(directory, filename)\n",
        "\n",
        "            # Read the contents of the file\n",
        "            with open(file_path, encoding='utf-8') as file:\n",
        "              content = file.read()\n",
        "              title_match = re.search(r'title:\\s*([^#]+)', content)\n",
        "              title = title_match.group(1) if title_match else \"\"\n",
        "              hidden_index = content.find(\"hidden:\")\n",
        "              if hidden_index != -1:\n",
        "                  content = content[hidden_index+12:].strip()\n",
        "              content = title + content\n",
        "            # Split the content into statement and solution\n",
        "              parts = content.split(\"## @lc code=start\")\n",
        "              if len(parts) == 2:\n",
        "                  statement = parts[0].strip().replace(\"## @lc code=start\", \"\").replace(\"#\", \"\").strip()\n",
        "                  solution = parts[1].strip().replace(\"## @lc code=end\", \"\").replace(\"using LeetCode\", \"\").strip()\n",
        "\n",
        "                  # Write to the CSV file\n",
        "                  writer.writerow({'statement': statement, 'solution': solution})\n",
        "\n",
        "print(f\"CSV file '{csv_file_path}' has been created successfully.\")\n"
      ],
      "metadata": {
        "id": "iJojSDIzoWf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test cases\n",
        "This section might have errors while running\n",
        "\n",
        "Some cases must be adressed specifically\n",
        "\n"
      ],
      "metadata": {
        "id": "UHkdLiwUoqcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_test_cases(s):\n",
        "    pattern = r'@testset \"(?:\\d+\\.)(.*?)\" begin(.*?)end'\n",
        "    matches = re.findall(pattern, s, re.DOTALL)\n",
        "    results = []\n",
        "    for match in matches:\n",
        "        title = match[0][:len(match[0])-3]\n",
        "        test_cases = re.findall(r'@test\\s(.*?)==\\s(.*?)\\n', match[1])\n",
        "        results.append({'title': title, 'test_cases': test_cases})\n",
        "    return results"
      ],
      "metadata": {
        "id": "4CYtnmt1otId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir = \"/content/LeetCode.jl/test/problems\"\n",
        "# Create or open the CSV file\n",
        "csv_file_path = \"Julia_leet_test.csv\"\n",
        "with open(csv_file_path, mode='w', newline='', encoding='utf-8') as csv_file:\n",
        "    fieldnames = ['title', 'test_case', 'expected_solution']\n",
        "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "\n",
        "    for filename in os.listdir(dir):\n",
        "        if filename.endswith(\".jl\"):\n",
        "            file_path = os.path.join(dir, filename)\n",
        "            with open(file_path, encoding='utf-8') as file:\n",
        "                content = file.read()\n",
        "                results = extract_test_cases(content)\n",
        "\n",
        "                for :\n",
        "                    writer.writerow({'title': , 'test_case': , 'expected_solution': })\n",
        "\n",
        "print(f\"CSV file '{csv_file_path}' has been created successfully.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "7b80S92EpG9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test cases\n",
        "s1 = '''@testset \"10.regular-expression-matching.jl\" begin\n",
        "    @test is_match(\"aa\", \"a\") == false\n",
        "    @test is_match(\"aa\", \"a*\") == true\n",
        "    @test is_match(\"ab\", \".*\") == true\n",
        "    @test is_match(\"aab\", \"c*a*b\") == true\n",
        "    @test is_match(\"mississippi\", \"mis*is*p*.\") == false\n",
        "    @test is_match(\"aab\", \"c.\") == false\n",
        "    @test is_match(\"\", \"c\") == false\n",
        "end'''\n",
        "\n",
        "s2 = '''@testset \"1.two-sum.jl\" begin\n",
        "    @test two_sum([2, 7, 11, 15], 9) == (1, 2)\n",
        "    @test two_sum([3, 2, 4], 6) == (2, 3)\n",
        "    @test two_sum([3, 3], 6) == (1, 2)\n",
        "end'''\n",
        "\n",
        "results1 = extract_test_cases(s1)\n",
        "print(\"Test set 1:\")\n",
        "for result in results1:\n",
        "    print(\"Title:\", result['title'])\n",
        "    print(\"Test cases:\")\n",
        "    for case in result['test_cases']:\n",
        "        print(case)\n",
        "\n",
        "results2 = extract_test_cases(s2)\n",
        "print(\"\\nTest set 2:\")\n",
        "for result in results2:\n",
        "    print(\"Title:\", result['title'])\n",
        "    print(\"Test cases:\")\n",
        "    for case in result['test_cases']:\n",
        "        print(case)\n"
      ],
      "metadata": {
        "id": "QXSFFRIcpQJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results1"
      ],
      "metadata": {
        "id": "tf08n08WpSF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation"
      ],
      "metadata": {
        "id": "ijQShrl3n-yJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "API_URL = \"https://api-inference.huggingface.co/models/philschmid/bart-large-cnn-samsum\"\n",
        "headers = {\"Authorization\": \"Bearer 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\"}"
      ],
      "metadata": {
        "id": "nZFX9BStnv4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize(input) :\n",
        "  API_URL = \"https://api-inference.huggingface.co/models/philschmid/bart-large-cnn-samsum\"\n",
        "  headers = {\"Authorization\": \"Bearer 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\"}\n",
        "\n",
        "  def query(payload):\n",
        "\t  response = requests.post(API_URL, headers=headers, json=payload)\n",
        "\t  return response.json()\n",
        "\n",
        "\n",
        "\n",
        "  output = query({\n",
        "    \"inputs\" : input\n",
        "  })\n",
        "  return output[0]['summary_text']"
      ],
      "metadata": {
        "id": "gBmymPfJn0Za"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the CSV file\n",
        "df = pd.read_csv('Julia_leet.csv')\n",
        "\n",
        "# Extract statements and process them\n",
        "processed_statements = []\n",
        "for statement in tqdm(df['statement']):\n",
        "    # Find the first occurrence of the word \"example\" (case insensitive)\n",
        "    match = re.search(r'\\bexample\\b', statement, flags=re.IGNORECASE)\n",
        "    if match:\n",
        "        # Extract the part of the statement before the match\n",
        "        processed_statement = statement[:match.start()].strip()\n",
        "    else:\n",
        "        # If \"example\" is not found, use the whole statement\n",
        "        processed_statement = statement.strip()\n",
        "    # Summarize the processed statement\n",
        "    summarized_statement = summarize(processed_statement)\n",
        "    processed_statements.append(summarized_statement)\n",
        "\n",
        "# Add the summarized statements to the DataFrame\n",
        "df['summarized_statement'] = processed_statements\n",
        "\n",
        "# Write the DataFrame to a new CSV file\n",
        "df.to_csv('summarized_data.csv', index=False)"
      ],
      "metadata": {
        "id": "sdnmRJEtn3Bm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}